{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/forchang33/Documents/github/dict.txt-v2.big ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with data1.json\n",
      "Input Stop Words\n",
      "Start Jieba Cut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /var/folders/5m/_hgd7lfd0_dbtcpyf2_9kmbr0000gn/T/jieba.u7331bff070b4114bdabf794a1974d279.cache\n",
      "Loading model cost 4.488 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Tf-Idf\n",
      "Sorted the Weight of KeyWords\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "###  ====================== 後端測試區 - TF-IDF =========================\n",
    "import json\n",
    "import jieba\n",
    "import re\n",
    "import os  \n",
    "import sys \n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 載入要parse的資料\n",
    "allData = []\n",
    "for dataName in range(1,2):    \n",
    "    with open('news.json'.format(dataName), 'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        print('Deal with data{}.json'.format(dataName))\n",
    "        allData += data\n",
    "### 載入繁體擴充字典\n",
    "jieba.set_dictionary('dict.txt-v2.big')    \n",
    "\n",
    "### 載入停用字\n",
    "print('Input Stop Words')\n",
    "stopwordset = set()\n",
    "with open('stopwords.txt','r',encoding='utf-8') as sw:\n",
    "    for line in sw:\n",
    "        stopwordset.add(line.strip('\\n'))\n",
    "\n",
    "print('Start Jieba Cut')\n",
    "totalContent = []\n",
    "### 從data中挑1000篇文章切詞 - 特別注意，這行的迴圈數字決定參予tf-idf計算的文章數量\n",
    "for i in range(0,50):\n",
    "    article = ''\n",
    "    content = allData[i]['Content']\n",
    "    \n",
    "    ### 斷詞, 產生一個結巴物件\n",
    "    words = jieba.cut(content, cut_all=False)\n",
    "    for word in words:   \n",
    "        ### 正規表達式，只針對文字處理\n",
    "        m = re.match(r'^[\\u4E00-\\u9FFFa-zA-Z]+$',word )\n",
    "        if m is not None:\n",
    "            if word not in stopwordset:\n",
    "                article+=word\n",
    "                article+=' '\n",
    "    totalContent.append(article)\n",
    "\n",
    "## 用來裝全部文章的權重值\n",
    "textWeightList = []\n",
    "\n",
    "## 用來裝前5權重值\n",
    "tagList = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TF-IDF 開始計算\n",
    "print('Start Tf-Idf')\n",
    "if __name__ == \"__main__\":  \n",
    "   \n",
    "    tfIdfVectorizer = TfidfVectorizer()    \n",
    "    tfIdf = tfIdfVectorizer.fit_transform(totalContent)    \n",
    "    ## 取得詞袋模型中的所有詞語 \n",
    "    myWord=tfIdfVectorizer.get_feature_names()        \n",
    "    ## 將tf-idf矩陣抽取出來， j詞在第i篇文章中的tf-idf權重 \n",
    "    weight=tfIdf.toarray()        \n",
    "    for i in range(len(weight)):\n",
    "        ## 用來裝單篇權重值\n",
    "        textMining = {}       \n",
    "        for j in range(len(myWord)):             \n",
    "            ##print( myWord[j],weight[i][j] ) \n",
    "            textMining[myWord[j]] = weight[i][j]\n",
    "        textWeightList.append(textMining)\n",
    "\n",
    "\n",
    "print('Sorted the Weight of KeyWords')\n",
    "### 權重排序 lanbda式\n",
    "for oneArticle in textWeightList:\n",
    "    dict= sorted(oneArticle.items(), key=lambda d:d[1], reverse = True)\n",
    "\n",
    "    ### 決定要放幾個tag\n",
    "    tag = []\n",
    "    for i in range(0,5):\n",
    "        tag.append(dict[i][0])\n",
    "    tagList.append(tag)\n",
    "        \n",
    "### 載入要parse的資料\n",
    "for i in range(len(tagList)):\n",
    "    allData[i]['Tag'] =tagList[i]\n",
    "\n",
    "\n",
    "\n",
    "## 讀取檔案\n",
    "listName = []\n",
    "\n",
    "with open('nameList.txt','r',encoding='utf-8') as nl:\n",
    "    for line in nl:\n",
    "        name = line.split('\\n')[0]\n",
    "        listName.append(name.lower())\n",
    "\n",
    "## 設定正規表達式\n",
    "reRule = '[a-zA-Z{\" \"}{\\.}{\\,}{\\-}a-zA-Z]+'\n",
    "for i in range(0,len(ç)):\n",
    "    tagByName = []\n",
    "    oneNew = allData[i]['Content']\n",
    "    ## 透過正規表達留下英文詞彙\n",
    "    reResult=re.findall(reRule, oneNew)\n",
    "    for j in reResult:\n",
    "        ## 比對名字是否為mlb球員\n",
    "        if j.lower() in listName:\n",
    "            tagByName.append(j.lower())\n",
    "    allData[i]['TagByName'] = tagByName"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
